Awesome Pipeline
================

A curated list of awesome pipeline toolkits inspired by [Awesome Sysadmin](https://github.com/kahun/awesome-sysadmin)

Pipeline frameworks & libraries
--------------------------------

* [ActionChain](http://docs.stackstorm.com/actionchain.html) - A workflow system for simple linear success/failure workflows.
* [Adage](https://github.com/diana-hep/adage) - Small package to describe workflows that are not completely known at definition time.
* [Airflow](https://github.com/airbnb/airflow) - Python-based workflow system created by AirBnb.
* [Anduril](http://www.anduril.org/anduril/site/) - Component-based workflow framework for scientific data analysis.
* [Antha](https://www.antha-lang.org/) - High-level language for biology.
* [Bds](http://pcingola.github.io/BigDataScript/) - Scripting language for data pipelines.
* [BioMake](https://github.com/evoldoers/biomake) - GNU-Make-like utility for managing builds and complex workflows.
* [BioQueue](https://github.com/liyao001/BioQueue) - Explicit framework with web monitoring and resource estimation.
* [Bistro](https://github.com/pveber/bistro) - Library to build and execute typed scientific workflows.
* [Bpipe](https://github.com/ssadedin/bpipe/) - Tool for running and managing bioinformatics pipelines.
* [Briefly](https://github.com/bloomreach/briefly) - Python Meta-programming Library for Job Flow Control.
* [Cluster Flow](http://clusterflow.io) - Command-line tool which uses common cluster managers to run bioinformatics pipelines.
* [Clusterjob](https://github.com/monajemi/clusterjob) - Automated reproducibility, and hassle-free submission of computational jobs to clusters.
* [Compss](https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar) - Programming model for distributed infrastructures.
* [Conan2](https://github.com/tburdett/Conan2) - Light-weight workflow management application.
* [Consecution](https://github.com/robdmc/consecution) - A Python pipeline abstraction inspired by Apache Storm topologies.
* [Cosmos](https://cosmos.hms.harvard.edu) - Python library for massively parallel workflows.
* [Cromwell](https://github.com/broadinstitute/cromwell) - Workflow Management System geared towards scientific workflows from the Broad Institute.
* [Cuneiform](https://github.com/joergen7/cuneiform) - Advanced functional workflow language and framework, implemented in Erlang.
* [Dagobah](https://github.com/thieman/dagobah) - Simple DAG-based job scheduler in Python.
* [Dagr](https://github.com/fulcrumgenomics/dagr) - A scala based DSL and framework for writing and executing bioinformatics pipelines as Directed Acyclic GRaphs.
* [Dask](https://github.com/dask/dask) - Dask is a flexible parallel computing library for analytics.
* [Dockerflow](https://github.com/googlegenomics/dockerflow) - Workflow runner that uses Dataflow to run a series of tasks in Docker.
* [Doit](http://pydoit.org/) - Task management & automation tool.
* [Drake](https://github.com/Factual/drake) - Robust DSL akin to Make, implemented in Clojure.
* [Dray](https://github.com/CenturyLinkLabs/dray) - An engine for managing the execution of container-based workflows. 
* [Flex](https://github.com/druths/flex/) - Language agnostic framework for building flexible data science pipelines (Python/Shell/Gnuplot).
* [Flowr](https://github.com/sahilseth/flowr) - Robust and efficient workflows using a simple language agnostic approach (R package).
* [Gwf](https://github.com/mailund/gwf) - Make-like utility for submitting workflows via qsub.
* [Hive](https://github.com/Ensembl/ensembl-hive) - System for creating and running pipelines on a distributed compute resource.
* [Joblib](https://pythonhosted.org/joblib/index.html) - Set of tools to provide lightweight pipelining in Python.
* [Jug](https://jug.readthedocs.io) - A task Based parallelization framework for Python.
* [Ketrew](https://github.com/hammerlab/ketrew) - Embedded DSL in the OCAML language alongside a client-server management application. 
* [Kronos](https://github.com/jtaghiyar/kronos) - Workflow assembler for cancer genome analytics and informatics.
* [Loom](https://github.com/StanfordBioinformatics/loom) - Tool for running bioinformatics workflows locally or in the cloud.
* [Longbow](http://www.hecbiosim.ac.uk/longbow) - Job proxying tool for biomolecular simulations.
* [Luigi](https://github.com/spotify/luigi) - Python module that helps you build complex pipelines of batch jobs.
* [Makeflow](http://ccl.cse.nd.edu/software/makeflow/) - Workflow engine for executing large complex workflows on clusters.
* [Mario](https://github.com/intentmedia/mario) - Scala library for defining data pipelines.
* [Mistral](https://github.com/openstack/mistral) - Python based workflow engine by the Open Stack project.
* [Moa](https://github.com/mfiers/Moa) - Lightweight workflows in bioinformatics.
* [Nextflow](http://www.nextflow.io) - Flow-based computational toolkit for reproducibile and scalable bioinformatics pipelines.
* [NiPype](https://github.com/nipy/nipype) - Workflows and interfaces for neuroimaging packages.
* [OpenGE](https://github.com/adaptivegenome/openge) - Accelerated framework for manipulating and interpreting high-throughput sequencing data.
* [Pachyderm](https://www.pachyderm.io/pps.html) - A containerized, version-controlled data lake.
* [PipEngine](https://github.com/fstrozzi/bioruby-pipengine) Ruby based launcher for complex biological pipelines.
* [Pinball](https://github.com/pinterest/pinball) - Python based workflow engine by Pinterest.
* [PyFlow](https://github.com/Illumina/pyflow) - Lightweight parallel task engine.
* [PypeFlow](https://github.com/PacificBiosciences/pypeFLOW) - Lightweight workflow engine for data analysis scripting.
* [pyperator](https://github.com/baffelli/pyperator) - Simple push-based python workflow framework using asyncio, supporting recursive networks.
* [pyppl](https://github.com/pwwang/pyppl) - A python lightweight pipeline framework.
* [pypyr](https://github.com/pypyr/pypyr-cli) - Simple task runner for sequential steps defined in a pipeline yaml, with AWS and Slack plug-ins.
* [Pwrake](https://github.com/masa16/Pwrake/) - Parallel workflow extension for Rake.
* [Qdo](https://bitbucket.org/berkeleylab/qdo) - Lightweight high-throughput queuing system for workflows with many small tasks to perform.
* [Qsubsec](https://github.com/alastair-droop/qsubsec) - Simple tokenised template system for SGE.
* [Rabix](https://github.com/rabix/rabix) - Python-based workflow toolkit based on the Common Workflow Language and Docker.
* [Remake](https://github.com/richfitz/remake) - Make-like declarative workflows in R.
* [Rmake](http://physiology.med.cornell.edu/faculty/mason/lab/r-make/) - Wrapper for the creation of Makefiles, enabling massive parallelization.
* [Rubra](https://github.com/bjpop/rubra) - Pipeline system for bioinformatics workflows.
* [Ruffus](http://www.ruffus.org.uk) - Computation Pipeline library for Python.
* [Ruigi](https://github.com/kirillseva/ruigi) - Pipeline tool for R, inspired by Luigi.
* [Sake](http://tonyfischetti.github.io/sake/) - Self-documenting build automation tool.
* [SciLuigi](https://github.com/pharmbio/sciluigi) - Helper library for writing flexible scientific workflows in Luigi.
* [SciPipe](http://scipipe.org) - Library for writing Scientific Workflows in Go.
* [Scoop](https://github.com/soravux/scoop/) - Scalable Concurrent Operations in Python.
* [Snakemake](https://bitbucket.org/johanneskoester/snakemake/wiki/Home) - Tool for running and managing bioinformatics pipelines.
* [Spiff](https://github.com/knipknap/SpiffWorkflow) - Based on the Workflow Patterns initiative and implemented in Python.
* [Stpipe](http://ssb.stsci.edu/doc/jwst_dev/jwst_lib.stpipe.doc/html/) - File processing pipelines as a Python library.
* [Suro](https://github.com/Netflix/suro) - Java-based distributed pipeline from Netflix.
* [Swift](http://swift-lang.org) - Fast easy parallel scripting - on multicores, clusters, clouds and supercomputers.
* [Toil](https://github.com/BD2KGenomics/toil) - Distributed pipeline workflow manager (mostly for genomics).
* [Yap](http://opensource.nibr.com/yap/) - Extensible parallel framework, written in Python using OpenMPI libraries.
* [WorldMake](http://worldmake.org/) - Easy Collaborative Reproducible Computing.
 
Workflow platforms 
--------------------
* [ActivePapers](http://www.activepapers.org/) - Computational science made reproducible and publishable.
* [Apache Iravata](https://airavata.apache.org/) - Framework for executing and managing computational workflows on distributed computing resources.
* [Arvados](http://arvados.org) - A container based workflow platform.
* [Biokepler](http://www.biokepler.org) - Bioinformatics Scientific Workflow for Distributed Analysis of Large-Scale Biological Data.
* [Chipster](http://chipster.csc.fi) - Open source platform for data analysis. 
* [Digdag](https://www.digdag.io) - Workflow manager designed for simplicity, extensibility and collaboration.
* [Fireworks](https://github.com/materialsproject/fireworks) - Centralized workflow server for dynamic workflows of high-throughput computations.
* [Galaxy](https://usegalaxy.org) - Web-based platform for biomedical research.
* [Kepler](https://kepler-project.org/) - Kepler scientific workflow application from University of California.
* [KNIME Analytics Platform](https://www.knime.org/knime-analytics-platform) - General-purpose platform with many specialized domain extensions.
* [NextflowWorkbench](http://workflow.campagnelab.org) - Integrated development environment for Nextflow, Docker and Reusable Workflows.
* [OpenMOLE](http://www.openmole.org/current/) - Workflow Management System for exploration of models and parameter optimization.
* [Ophidia](http://ophidia.cmcc.it) - Data-analytics platform with declarative workflows of distributed operations.
* [Pegasus](http://pegasus.isi.edu) - Workflow Management System.
* [Pentaho Kettle](http://community.pentaho.com/projects/data-integration/) - Workflow platform with a graphical design environment.
* [Sushi](https://github.com/uzh/sushi) - Supporting User for SHell script Integration.
* [Yabi](http://ccg.murdoch.edu.au/yabi) - Online research environment for grid, HPC and cloud computing.
* [Taverna](http://www.taverna.org.uk) - Domain independent workflow system.
* [VisTrails](http://www.vistrails.org/) - Scientific workflow and provenance management system.
* [Wings](http://www.wings-workflows.org) - Semantic workflow system utilizing Pegasus as execution system.

Workflow languages 
-------------------
* [Common Workflow Language](https://github.com/common-workflow-language/common-workflow-language)
* [Cloudgene Workflow Language](http://cloudgene.uibk.ac.at/developer-guide)
* [OpenMOLE DSL](http://www.openmole.org/current/Documentation_Language.html)
* [Workflow Definition Language](https://github.com/broadinstitute/wdl)
* [Yet Another Workflow Language](http://www.yawlfoundation.org)

Workflow standardization initiatives
---------------------------
* [Workflow 4 Ever Initiative](http://www.wf4ever-project.org)
* [Workflow 4 Ever workflow research object model](http://wf4ever.github.io/ro)
* [Workflow Patterns Initiative](http://www.workflowpatterns.com)
* [Workflow Patterns Library](http://www.workflowpatterns.com/patterns)
* [ResearchObject.org](http://www.researchobject.org)

Literate programming (aka interactive notebooks) 
---------------------------------------------------
* [Beaker](http://beakernotebook.com/) Notebook-style development environment. 
* [Binder](http://mybinder.org/) - Turn a GitHub repo into a collection of interactive notebooks powered by Jupyter and Kubernetes
* [IPython](https://ipython.org/) A rich architecture for interactive computing.
* [Jupyter](https://jupyter.org/) Language-agnostic notebook literate programming environment.
* [Pathomx](http://pathomx.org) - Interactive data workflows built on Python.
* [R Notebooks](http://rmarkdown.rstudio.com/r_notebooks.html) - R Markdown notebook literate programming environment.
* [Zeppelin](http://zeppelin-project.org/) - Web-based notebook that enables interactive data analytics.



Build automation tools
----------------------
* [Bazel](http://bazel.io/) - Build software just as engineers do at Google.
* [DoIt](https://github.com/pydoit/doit) - Highly generalized task-management and automation in Python.
* [Gradle](http://gradle.org/) - Unified cross platforms builds.
* [Scons](http://www.scons.org/) - Python library focused on C/C++ builds.
* [Shake](https://github.com/ndmitchell/shake) - Define robust build systems akin to GNU Make using Haskell.
* [Make](https://www.gnu.org/software/make/) - The GNU Make build system.


Other projects
----------------
* [HPC Grid Runner](http://hpcgridrunner.github.io/)
* [noWorkflow](https://github.com/gems-uff/noworkflow) - Supporting infrastructure to run scientific experiments without a scientific workflow management system, and still get things like provenance.
* [Reprozip](https://github.com/ViDA-NYU/reprozip) - Simplifies the process of creating reproducible experiments from command-line executions.


Related lists
--------------
* [Awesome streaming](https://github.com/manuzhang/awesome-streaming) - Curated list of awesome streaming frameworks, applications.
* [Awesome ETL](https://github.com/pawl/awesome-etl) - Curated list of notable ETL (extract, transform, load) frameworks, libraries and software.
* [Computational Data Analysis Workflow Systems](https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems)

